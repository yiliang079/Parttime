#### fp32 fp16 bf16数值

------

**常见的浮点类型有fp16，fp32，bf16，tf32，fp24，pxr24，ef32，能表达的数据范围主要看exponent，精度主要看fraction。**

可以看出表达的数据范围看fp32，bf16，tf32，pxr24和ef32都是一样的，因为大家能表达的都是-2^254 ~ 2^255 这个大概范围。fp24到表达范围比上面这些小，是 -2^126 ~ 2^127 

从精度上看fp32 > pxr24 > ef32 > tf32 > bf16，燧原的ef32都精度比Nvidia的fp32的精度多了一位，但都显著优于google的bf16，燧原的ef32，Nvidia的tf32都是使用fp16的计算性能像fp32的表达范围靠齐的一种尝试。

------

##### fp32

<img src="pic/image-20220208175922240.png" alt="image-20220208175922240" style="zoom:150%;" />

------

##### fp16

![image-20220208175956221](pic/image-20220208175956221-4314397.png)

------

##### bf16

‘BF16’，专为人工智能(AI)/深度学习(DL)应用优化发展而来，有时也称‘BFloat16’或‘Brain Float 16’。它一开始是由Google Brain团队发明，并用于其第三代Tensor Processing Unit (TPU)，如今已被Google、英特尔(Intel)、Arm等许多公司的AI加速器广泛采用。

采用16位脑浮点(brain floating point)格式的BF16，主要概念在于透过降低数字的精度，从而减少让张量(tensor)相乘所需的运算资源和功耗。「张量」是数字的三维(3D)矩阵；张量的乘法运算即是AI计算所需的关键数学运算。

如今，大多数的AI训练都使用FP32，即32位浮点数。尽管这表示可以达到非常准确的计算，但需要强大的硬件而且极其耗电。推论一般使用INT8，即8位整数精度的运算模式，虽然是较低精度的数字系统，但在相同硬件上提供了更高的传输效率，因而能够更省电，只是计算结果(预测)的准确性较低些。

**BF16的基本概念是为精度和预测准确性之间的权衡进行优化，从而提高吞吐量。**

![img](pic/2508854-20210818154056839-671903724.png)